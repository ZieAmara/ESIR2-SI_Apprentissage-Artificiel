{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:40:30.389329Z",
     "start_time": "2021-11-23T13:40:27.788833Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aa_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m norm\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m multivariate_normal\n\u001b[0;32m---> 13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39maa_utils\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aa_utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "import aa_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font  style=\"font-size: 4rem; color: #1abc9c\"> Bayesian Learning </font>\n",
    "\n",
    "## Table of Contents\n",
    "* [Exercise 1: Gaussian Naive Bayes](#exo1)\n",
    "    * [Naive Bayes exploration](#section_1_1)\n",
    "    * [Visualization](#section_1_2)\n",
    "    * [Comparison with KDE](#section_1_3)\n",
    "    * [Evaluation](#section_1_4)\n",
    "* [Exercise 2: k-Nearest Neighbors](#exo2)\n",
    "    * [Basic k-NN](#section_2_1)\n",
    "    * [Data preprocessing](#section_2_2)\n",
    "    * [Select $k$ with a validation set](#section_2_3)\n",
    "* [Appendix: KDE approfondissement](#exo3)\n",
    "    * [1D KDE (Naive Bayes hypothesis)](#section_3_1)\n",
    "    * [2D KDE (Multivariate KDE)](#section_3_2)\n",
    "    * [K-nn cleaning and condensing](#section_3_2)\n",
    "    \n",
    "\n",
    "\n",
    "# <font color=\"#1E90FF\">Exercise 1: Gaussian Naive Bayes</font><a class=\"anchor\" id=\"exo1\"></a>\n",
    "\n",
    "In Scikit-learn, the Gaussian Naive Bayes algorithm for classification is implemented via the <code>GaussianNB</code> class. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:09:47.669168Z",
     "start_time": "2021-11-23T13:09:47.489235Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_exam.txt', sep = ' ')\n",
    "print(df)\n",
    "dataset = df.rename(columns={df.columns[0]: 'X1',df.columns[1]: 'X2',df.columns[2]: 'Y'})\n",
    "sns.scatterplot(data=dataset, x='X1', y='X2', hue='Y', marker='+', palette=['blue','red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:10:46.105731Z",
     "start_time": "2021-11-23T13:10:46.068184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset split into train/test set\n",
    "data_train, data_test = train_test_split(dataset, test_size = 0.3, random_state = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#9400D3\">1. Gaussian Naive Bayes Exploration</font><a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "\n",
    "We learn a Gaussian Naive Bayes classifier on the training data, and analyze the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:10:52.107853Z",
     "start_time": "2021-11-23T13:10:52.000890Z"
    }
   },
   "outputs": [],
   "source": [
    "# learn a Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(data_train.loc[:, ['X1', 'X2']], data_train.Y)\n",
    "print('Probability of each class:', gnb.class_prior_)\n",
    "print('Variance of each feature per class:',gnb.sigma_)\n",
    "print('Mean of each feature per class:',gnb.theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the pdf (probability density function) for $x_1|w_1$ and $x_1|w_2$.\n",
    "\n",
    "<font color=\"blue\">**TODO:**</font> : Plot the pdf for $x_2|w_1$ and $x_2|w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_note1_w1 = data_train[data_train.Y == 0].iloc[:, 0].to_numpy()\n",
    "np_note1_w2 = data_train[data_train.Y == 1].iloc[:, 0].to_numpy()\n",
    "\n",
    "x = np.linspace(0, 120, 100)\n",
    "# Plot density for class w1=0\n",
    "density_w1 = norm.pdf(x, gnb.theta_[0, 0], np.sqrt(gnb.sigma_[0, 0]))\n",
    "plt.plot(x, density_w1, color = \"blue\", linestyle = \"-\",label='gaussian class 0')\n",
    "plt.hist(np_note1_w1, density=True, color='blue', alpha=0.2, label='hist class 0')\n",
    "plt.scatter(np_note1_w1, np.zeros(len(np_note1_w1)), color = \"blue\")\n",
    "# Plot density for class w2=1\n",
    "density_w2 = norm.pdf(x, gnb.theta_[1, 0], np.sqrt(gnb.sigma_[1, 0]))\n",
    "plt.plot(x, density_w2, color = \"red\", linestyle = \"-\",label='gaussian class 1')\n",
    "plt.hist(np_note1_w2, density=True, color='red', alpha=0.2, label='hist class 1')\n",
    "plt.scatter(np_note1_w2, np.zeros(len(np_note1_w2)), color = \"red\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Plot for x2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Question 1:**</font> Compute the class of the first example of the test set. **You will detail the calculation in your report, indicating the formulas (math) used**, before giving their numerical values.\n",
    "\n",
    "**Tips**:\n",
    "- $P(x_i|w_j)$ is given by <code>norm.pdf</code>($x_i, \\mu_j, \\sigma_j$). Check the given code above that plots the densities.\n",
    "- You can verify your computations using the fonction <code>predict_proba</code>. \n",
    "\n",
    "Usage : <code>gnb.predict_proba(data_test.iloc[0,:2].values.reshape(1, -1))</code>\n",
    "- To end up with the same values, you should compute the 'real' value of $P(w|x)$, which means normalizing by $P(x)$, even if this is not needed in practice to classify the data.\n",
    "\n",
    "Remember your probas: $P(x)=P(x|w_1) \\times P(w_1) + P(x|w_2) \\times P(w_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:16:03.978738Z",
     "start_time": "2021-11-23T13:16:03.941779Z"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO - 'manually' compute the class of the first example in the test set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#9400D3\">2. Vizualisation</font><a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the gaussian NB likelihood\n",
    "aa_utils.draw_gaussianNB_likelihood(gnb,data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the NB classifier decision boundaries\n",
    "aa_utils.draw_decision_boundaries(gnb,data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Naive Bayes predicted probabilities for class w2\n",
    "aa_utils.draw_predicted_probabilities(gnb,data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Question 2:**</font> For each of the figures below explain what is represented. You will indicate in each case the corresponding mathematical formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#9400D3\">3. Comparison with KDE</font><a class=\"anchor\" id=\"section_1_3\"></a>\n",
    "\n",
    "We now compute the likelihood by Kernel Density Estimation with a 'gaussian' kernel implemented in scikit-learn by <code>KernelDensity</code> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:19:31.575898Z",
     "start_time": "2021-11-23T13:19:31.517688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot density for class w1=0\n",
    "gkde_w1 = KernelDensity(kernel='gaussian', bandwidth=5).fit(np_note1_w1.reshape(-1, 1))\n",
    "g_log_dens_w1 = gkde_w1.score_samples(x.reshape(-1, 1))\n",
    "plt.plot(x,np.exp(g_log_dens_w1), color=\"blue\",linestyle=\"--\", label='gaussian kde class 0')\n",
    "plt.hist(np_note1_w1, density=True, color='blue', alpha=0.2, label='hist class 0')\n",
    "plt.scatter(np_note1_w1, np.zeros(len(np_note1_w1)), color = \"blue\")\n",
    "# Plot density for class w2=1\n",
    "gkde_w2 = KernelDensity(kernel='gaussian', bandwidth=5).fit(np_note1_w2.reshape(-1, 1))\n",
    "g_log_dens_w2 = gkde_w2.score_samples(x.reshape(-1, 1))\n",
    "plt.plot(x,np.exp(g_log_dens_w2), color=\"red\",linestyle=\"--\", label='gaussian kde class 1')\n",
    "plt.hist(np_note1_w2, density=True, color='red', alpha=0.2, label='hist class 1')\n",
    "plt.scatter(np_note1_w2, np.zeros(len(np_note1_w2)), color = \"red\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Question 3:**</font> \n",
    "Explain the difference between the estimation of $P(x|w)$ by a gaussian pdf, and the gaussian KDE.\n",
    "\n",
    "<font color=\"red\">**Question 4:**</font> Change the value of the <code>bandwith</code> parameter (give a float value). Conclude on the influence of this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than assuming attributes independance and use 1D density estimation, KDE can advantageously be used on all the attributes. In our case, the kernel used is a 2D gaussian parametrized by the bandwith $h$.\n",
    "\n",
    "<font color=\"blue\">**TODO:**</font> Observe below the difference with the likelihood estimated by the gaussian NB. You can change the bandwith parameter value to see the effects of this hyper-parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Multivariate KDE\n",
    "# bw_h is the bandwith parameter\n",
    "aa_utils.draw_sk_kde_densities(bw_h=9,data=data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#9400D3\">4. Evaluation</font><a class=\"anchor\" id=\"section_1_4\"></a>\n",
    "\n",
    "There is no implementation in scikit-learn of a Bayes Classifier using KDE.\n",
    "\n",
    "<font color=\"blue\">**TODO:**</font> Implement the KDE-based Bayes classifier.\n",
    "\n",
    "**Hints:** Following methods will be usefull:\n",
    "- <code>kde.score_samples(X)</code> : compute the log-likelihood of each sample under the model\n",
    "- to get the likelihood: <code>np.exp(kde.score_samples(X))</code>\n",
    "\n",
    "where <code>kde</code> is the Kernel Density estimator (model).\n",
    "\n",
    "<font color=\"red\">**Question 5:**</font> Compare **in a table** the performances in generalization of the gaussian NB and a KDE-based Bayes classifier in terms of accuracy or error, for different values of $h$ and different kernels.\n",
    "\n",
    "<code>KernelDensity</code> implements several common kernel forms. Refer to the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Performance of NB \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Bayes classification with 2D KDE and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#1E90FF\">Exercise 2: k-Nearest Neighbors</font><a class=\"anchor\" id=\"exo2\"></a>\n",
    "\n",
    "## <font color=\"#9400D3\">1. Basic k-NN</font>\n",
    "\n",
    "We will first look at the k-nn classifier with $k=1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:42:00.973287Z",
     "start_time": "2021-11-23T13:41:54.840303Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('./data/knn_dataset.csv', sep = ',')\n",
    "print(df)\n",
    "sns.scatterplot(data=df, x='X1', y='X2', hue='Y', marker='+', palette=['blue','red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "data_train, data_test = train_test_split(df, test_size = 0.3, random_state = 13)\n",
    "data_valid, data_test = train_test_split(data_test, test_size = 0.5, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN\n",
    "nn_clf = KNeighborsClassifier(n_neighbors=1) # this initializes a knn for k = 1\n",
    "nn_clf.fit(data_train.iloc[:,:-1],data_train.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Question 1:**</font> Which distance among samples is used to compute the nearest neighobors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:37:37.222681Z",
     "start_time": "2021-11-23T13:37:36.848963Z"
    }
   },
   "outputs": [],
   "source": [
    "# Affichage frontiere de decision\n",
    "aa_utils.draw_decision_boundaries(nn_clf,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knn classifier evaluation\n",
    "print('Accuracy test sample :', nn_clf.score(data_test.iloc[:,:-1], data_test.Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#9400D3\">2. Data preprocessing</font><a class=\"anchor\" id=\"section_2_2\"></a>\n",
    "\n",
    "When classifiers are based on distance calculations between examples, it may be important to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T14:03:54.572814Z",
     "start_time": "2021-11-23T14:03:49.150224Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "np_scaled=scaler.fit_transform(df.iloc[:,:-1])\n",
    "\n",
    "df_scaled = pd.DataFrame(np_scaled, columns=['X1','X2'])\n",
    "df_scaled['Y'] = df.Y\n",
    "sns.scatterplot(data=df_scaled, x='X1', y='X2', hue='Y', marker='+', palette=['blue','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Question 2:**</font> What is the effect of the pre-processing applied here?\n",
    "\n",
    "<font color=\"red\">**Question 3:**</font> \n",
    "- Compare the performance of the KNN classifier using pre-processed data with the previous one. \n",
    "- Explain why the classifier benefits from the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Evaluate the KNN classifier with pre-processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage frontiere de decision\n",
    "#aa_utils.draw_decision_boundaries(model,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#9400D3\">3. Select $k$ with a validation set</font><a class=\"anchor\" id=\"section_2_3\"></a>\n",
    "\n",
    "$k$ being an hyper-parameter of the KNN classifier, its value should be tuned using a validation set.\n",
    "\n",
    "<font color=\"blue\">**TODO:**</font> \n",
    "- Plot the validation error as a function of $k$\n",
    "- Select the best value of $k$\n",
    "- Evaluate the resulting classifier\n",
    "- Plot the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:42:03.521560Z",
     "start_time": "2021-11-23T13:42:03.257931Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO - Plot the validation error as a function of the number of neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best hyper-parameter k\n",
    "\n",
    "\n",
    "# Evaluate the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T13:39:19.561680Z",
     "start_time": "2021-11-23T13:39:19.541406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Affichage frontiere de decision\n",
    "#aa_utils.draw_decision_boundaries(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T14:03:31.178720Z",
     "start_time": "2021-11-23T14:03:29.414309Z"
    }
   },
   "source": [
    "<font color=\"red\">**Question 4:**</font> Compare the performances of the (gaussian) Naive Bayes and the (tuned) k-NN classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T14:03:56.940737Z",
     "start_time": "2021-11-23T14:03:56.690574Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO - NB and k-NN classifiers comparison\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
